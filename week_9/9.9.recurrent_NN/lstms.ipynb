{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTMs to carry out Sentiment Analysis on movie reviews\n",
    "### Steps:\n",
    "* Import the necessary packages\n",
    "* Create and Preprocess the data\n",
    "* Build the model\n",
    "* Train and Evaluate\n",
    "* Test on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the data\n",
    "#### Lets make up some of our own labelled movie reviews. \n",
    "#### Why don't we consider a classic separator of opinion, Tommy Wiseau's `The Room`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = ['I will never watch this movie because its so bad',\n",
    "          'The room is to filmmaking as Hugo Boss is to military clothing',\n",
    "          'This movie improved my life',\n",
    "          'The is the best movie Ive ever done',\n",
    "          'It was a pleasure watching for my kids and me',\n",
    "          'I hated this movie from start to finish',\n",
    "          'It was awful, just awful',\n",
    "          'It was really bad']\n",
    "\n",
    "labels = [0,1,1,1,1,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "#### Factorise the text\n",
    "* We have to transform the text we give to the sentiment analysis network\n",
    "* Do this by creating a vocab and vocab length\n",
    "* Use those to create a **vocab_to_keys** and **keys_to_vocab** dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 : Create the vocab and vocab length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "max_length = 0\n",
    "\n",
    "for review in reviews:\n",
    "    review = review.lower().split()\n",
    "    for word in review:\n",
    "        vocab.append(word)\n",
    "        if len(review) > max_length:\n",
    "            max_length = len(review)\n",
    "    \n",
    "vocab = list(set(vocab)) # set function cleans duplicates \n",
    "max_length = max_length +1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just',\n",
       " 'hated',\n",
       " 'it',\n",
       " 'me',\n",
       " 'as',\n",
       " 'watch',\n",
       " 'ever',\n",
       " 'watching',\n",
       " 'room',\n",
       " 'the',\n",
       " 'finish',\n",
       " 'done',\n",
       " 'so',\n",
       " 'to',\n",
       " 'will',\n",
       " 'kids',\n",
       " 'for',\n",
       " 'is',\n",
       " 'a',\n",
       " 'from',\n",
       " 'movie',\n",
       " 'really',\n",
       " 'clothing',\n",
       " 'its',\n",
       " 'start',\n",
       " 'awful',\n",
       " 'my',\n",
       " 'military',\n",
       " 'i',\n",
       " 'bad',\n",
       " 'awful,',\n",
       " 'improved',\n",
       " 'ive',\n",
       " 'never',\n",
       " 'filmmaking',\n",
       " 'life',\n",
       " 'because',\n",
       " 'best',\n",
       " 'pleasure',\n",
       " 'hugo',\n",
       " 'was',\n",
       " 'and',\n",
       " 'boss',\n",
       " 'this']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Create the two dictionaries - vocab to keys and keys to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding \n",
    "keys_to_vocab = {i:voc for i, voc in enumerate(vocab, start=1)}\n",
    "\n",
    "# encoding\n",
    "vocab_to_keys = {voc:i for i, voc in enumerate(vocab, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just\n",
      "1\n",
      "just\n"
     ]
    }
   ],
   "source": [
    "print(list(vocab_to_keys.keys())[0])\n",
    "print(vocab_to_keys['just'])\n",
    "print(keys_to_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Now we can factorize each review by turning it into a series of numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = [[vocab_to_keys[word] for word in review.lower().split()] for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[29, 15, 34, 6, 44, 21, 37, 24, 13, 30],\n",
       " [10, 9, 18, 14, 35, 5, 40, 43, 18, 14, 28, 23],\n",
       " [44, 21, 32, 27, 36],\n",
       " [10, 18, 10, 38, 21, 33, 7, 12],\n",
       " [3, 41, 19, 39, 8, 17, 27, 16, 42, 4],\n",
       " [29, 2, 44, 21, 20, 25, 14, 11],\n",
       " [3, 41, 31, 1, 26],\n",
       " [3, 41, 22, 30]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Finally, pad the review sequences so they are all the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_doc = sequence.pad_sequences(embedded_docs, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29, 15, 34,  6, 44, 21, 37, 24, 13, 30,  0,  0,  0],\n",
       "       [10,  9, 18, 14, 35,  5, 40, 43, 18, 14, 28, 23,  0],\n",
       "       [44, 21, 32, 27, 36,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [10, 18, 10, 38, 21, 33,  7, 12,  0,  0,  0,  0,  0],\n",
       "       [ 3, 41, 19, 39,  8, 17, 27, 16, 42,  4,  0,  0,  0],\n",
       "       [29,  2, 44, 21, 20, 25, 14, 11,  0,  0,  0,  0,  0],\n",
       "       [ 3, 41, 31,  1, 26,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 3, 41, 22, 30,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with the sequential initialiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then add the word embedding layer\n",
    "##### This layer takes 3 parameters - the size of the vocab (input_dims), the no. of dimensions of each word embedding (output_dim), and the length of each document (input_length), which we've standardised above. It returns a 2d matrix, with rows equal to each word in the document, and columns equal to the number of dimensions in the word embedding. \n",
    "\n",
    "*Actually its 3D, cos the batch_size is the first dimension in both input and output, but I find that confuses things more than it clarifies*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put another way \n",
    "\n",
    "The embedding **takes in** a factorized corpus, e.g.:\n",
    "\n",
    "**[The, cat, sat, on, the, mat]**    becomes    **[1,2,3,4,1,5]**\n",
    "\n",
    "And **outputs** a word embedded corpus:\n",
    "\n",
    "**[1,2,3,4,1,5]**    becomes (lets assume output_dim=2)   **[[0.2,0.7], [0.6,0.3], [0.1,0.8], [0.2,0.1], [0.2,0.7], [0.4,0.9]]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a big corpus, embedding will help us here\n",
    "model.add(Embedding(input_dim=len(vocab)+1, output_dim=32, input_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then add the LSTM layer\n",
    "* We have to define the units, which defines the number of recurrent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(128, use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, add a Dense layer, which has 1 neuron for our binary Positive/Negative classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 13, 32)            1440      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               81920     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 1, 2)              90        \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               66560     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 150,268\n",
      "Trainable params: 150,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can compile and fit the model on the training data\n",
    "* Fit the word embeddings from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_doc\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10,  9, 18, 19, 42, 10, 21, 10,  0,  0,  0,  0,  0],\n",
       "       [42, 13, 42,  3,  3, 14,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [29,  3, 41, 10, 38, 21, 33,  7,  0,  0,  0,  0,  0],\n",
       "       [ 3, 27,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 4\n  y sizes: 8\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/08/6zrs8w253573k0k0yjq1w49c0000gn/T/ipykernel_33467/1897432793.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1651\u001b[0m                            for i in tf.nest.flatten(single_data)))\n\u001b[1;32m   1652\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 4\n  y sizes: 8\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "model.fit(X,y, epochs=29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we're happy, we can try and guess the sentiment of some new text\n",
    "#### **Beware, the vocab has to include that which we've already used!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before giving it to the model, make sure to preprocess the text in the same way, and lets add some test_labels which we can compare against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reviews = ['The Room is a masterpiece and one of the most underappreciated movie of the modern era',\n",
    "              'Every line and scene has so much care and effort put into it that it easily puts many far more ambitious films to shame',\n",
    "              'I thought it was the best movie Ive ever seen',\n",
    "              'it made my eyes bleed',\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And predict the sentiment of the sentence against the model's prediction of the padded_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_docs = [[vocab_to_keys[word] for word in review.lower().split() if word in vocab] for review in new_reviews]\n",
    "padded_doc = sequence.pad_sequences(embedded_docs, maxlen=max_length, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_doc\n",
    "ypred = []\n",
    "\n",
    "ypred.append(model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Room is a masterpiece and one of the most underappreciated movie of the modern era',\n",
       " 'Every line and scene has so much care and effort put into it that it easily puts many far more ambitious films to shame',\n",
       " 'I thought it was the best movie Ive ever seen',\n",
       " 'it made my eyes bleed']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.86574066],\n",
       "        [0.333807  ],\n",
       "        [0.14551404],\n",
       "        [0.00281903]], dtype=float32)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
