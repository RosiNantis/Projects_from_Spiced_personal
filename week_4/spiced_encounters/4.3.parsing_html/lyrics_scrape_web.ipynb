{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b481b94a",
   "metadata": {},
   "source": [
    "## Using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c86cae0",
   "metadata": {},
   "source": [
    "BeautifulSoup is a library which parses HTML and creates a tree structure of python objects that we can navigate through, extract information from, and edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5682db35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -y bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347d2afb",
   "metadata": {},
   "source": [
    "#### Convert the raw HTML string to a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28ab01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a40f9a",
   "metadata": {},
   "source": [
    "### Task 1: Extract the poem titles, url and poems with Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0b1f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_url(url): \n",
    "    import os\n",
    "    import time\n",
    "    \"\"\"\n",
    "    search_url is a function that accepts as parameter a url string\n",
    "    and then returns a text file named as text with the web page\n",
    "    requested. \n",
    "    It prints in the excecustion if the code is corectly downloaded.\n",
    "    It uses request package!\n",
    "    \"\"\"                                                                                               \n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "    response = requests.get(url, headers=header)\n",
    "    time.sleep(3)\n",
    "    print(f'The status code is: {response.status_code}')\n",
    "    filename = f\"response_files/response.txt\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(response.text)\n",
    "    text = response.text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22d3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_songs(class_of,saving,artist):\n",
    "    \"\"\"\n",
    "    Makes a Data Frame with the title, url, and lyrics as columns.\n",
    "    As parameter you have to include the class of the list of songs on the url.\n",
    "    Be careful about the number of \\n in the title and extract them.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    all_song_titles = soup.find_all(class_= class_of)\n",
    "    all_song_titles\n",
    "    titles = []\n",
    "    for title in all_song_titles:\n",
    "        titles.append(title.a.text.split('\\n')[0].strip()) \n",
    "        \n",
    "    # Extract all links\n",
    "    links = []\n",
    "    songlinks = soup.find_all(class_= class_of)\n",
    "    for link in songlinks:\n",
    "        links.append(link.a['href'])\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({'title':titles,'links':links})\n",
    "    df['artist'] = artist\n",
    "    df['song_lyric'] = 'blank'\n",
    "    for i in range(len(df.links[:])):\n",
    "        try:\n",
    "            lyric = BeautifulSoup(requests.get(df.links[i]).text,)\n",
    "            df['song_lyric'][i] = lyric.find('p',attrs={'class':'song-lyrics'}).text\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            continue\n",
    "    if saving == True:\n",
    "        import os\n",
    "        for i in range(len(df.song_lyric[:])):\n",
    "            filename = f\"response_files//lyricsDOTcom/{df.title[i]}.txt\"\n",
    "            os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "            with open(filename,'w') as f:\n",
    "                f.write(df.song_lyric[i])\n",
    "                f.close()\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607f1090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The status code is: 200\n",
      "The status code is: 200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = search_url('https://lyrics.az/bob-dylan/allalbums.html')\n",
    "soup = BeautifulSoup(text,'html.parser')\n",
    "df1 = find_songs('mt-3 col-sm-9 col-12 list-group mt-sm-0',True,'bob_dylan')\n",
    "\n",
    "text = search_url('https://lyrics.az/eminem/allalbums.html')\n",
    "soup = BeautifulSoup(text,'html.parser')\n",
    "df2 = find_songs('mt-3 col-sm-9 col-12 list-group mt-sm-0',True,'eminem')\n",
    "\n",
    "# text = search_url('https://lyrics.az/simon-and-garfunkel/allsongs.html')\n",
    "# soup = BeautifulSoup(text,'html.parser')\n",
    "# df3 = find_songs('px-0 mx-0 mb-5 col-12 col-sm-6 table-responsive',True,'simon&granfunkel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af72897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data workspace!\n",
    "import os  \n",
    "os.makedirs('save_frames_artists', exist_ok=True)  \n",
    "df1.to_csv('save_frames_artists/bob_dylan_no_index.csv',index=False) \n",
    "df2.to_csv('save_frames_artists/eminem_no_index.csv',index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
